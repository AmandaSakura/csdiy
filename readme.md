### 大模型算法实习生学习计划 (3-4个月高强度)

#### 核心理念：
1.  **代码自主**: 在学习初期，严格禁止使用LLM生成完整的代码块。你可以用它来解释概念、调试错误，但绝不能用它直接写功能。**你必须亲手敲下每一行逻辑代码。**
2.  **实践驱动**: 每个理论学习阶段都必须伴随一个具体的、可交付的项目。没有跑通的代码，就没有真正的理解。
3.  **深度优先**: 与其泛泛了解10个技术，不如精通2个核心技术。我们将深度聚焦在`Transformer实现`和`LoRA微调`上。

---

### **第一阶段：代码能力重建与PyTorch基础 (持续4周)**

**目标**: 戒除对LLM的代码依赖，能独立使用PyTorch从零开始构建、训练和评估一个深度学习模型。

* **理论学习 (Week 1-2):**
    * **Git入门**: 学习使用`git`进行代码版本控制 (`clone`, `add`, `commit`, `push`)。
    * **Python环境管理**: 掌握`conda`或`venv`，学会创建独立的虚拟环境来管理项目依赖。
    * **精通PyTorch核心**:
        * `torch.Tensor`: 张量的创建、维度变换、数学运算。
        * `torch.autograd`: 理解自动求导机制，它是所有模型训练的基础。
        * `nn.Module`: 如何通过继承它来构建自己的模型。
        * `nn.Linear`, `nn.Embedding`, `nn.CrossEntropyLoss`: 掌握最核心的层和损失函数。
        * `torch.optim`: 熟悉`AdamW`等优化器的工作方式。
        * `Dataset` & `DataLoader`: 学会封装自己的数据集，并进行高效的数据加载。

* **项目实践 (Week 3-4): 从零实现一个简单的图像分类器**
    * **任务**: 不使用任何高级库（如`timm`），仅使用PyTorch，在`CIFAR-10`数据集上实现一个经典的卷积神经网络（如LeNet-5或一个简单的自定义CNN）。
    * **产出**:
        1.  一个能独立运行的Python脚本。
        2.  代码包含清晰的数据加载、模型定义、训练循环、评估循环四个部分。
        3.  训练过程能打印Loss和Accuracy，并能保存最好的模型权重。
    * **检验标准**: 你可以把所有代码删掉，仅凭记忆和查阅PyTorch官方文档，重新实现整个项目。

---

### **第二阶段：深入核心架构与微调技术 (持续6周)**

**目标**: 将理论知识转化为代码，亲手实现Transformer的核心组件，并掌握最主流的LoRA微调技术。

* **理论学习 (Week 5):**
    * **重温Transformer**: 再次精读论文《Attention Is All You Need》。这次的重点是理解每个组件的数学细节和伪代码。
    * **学习LoRA**: 阅读LoRA论文的摘要、引言和图示。理解其核心思想：**通过低秩分解，用两个小矩阵(A和B)来模拟对大矩阵的更新**，从而实现参数高效微调。

* **项目实践 1 (Week 6-7): 从零手写Multi-Head Attention**
    * **任务**: 禁止使用`nn.Transformer`或任何现成的实现。你需要用`nn.Linear`等基础模块，亲手构建一个功能完整的`Multi-Head Attention`模块。
    * **产出**: 一个自定义的`MultiHeadAttention`类，它能接受输入张量(Q, K, V)，并输出经过注意力计算后的结果。
    * **检验标准**: 你能给同学清晰地讲明白代码中每一步矩阵乘法的维度变化及其物理意义。

* **项目实践 2 (Week 8-10): 使用Hugging Face完成一次完整的LoRA微调**
    * **任务**: 在你的4060笔记本上，使用`Hugging Face`全家桶 (`transformers`, `datasets`, `peft`)，对一个开源小模型（如`Qwen-1.8B`, `Gemma-2B`）进行QLoRA微调。
    * **数据集**: 选择一个公开的、简单的指令微调数据集（如`alpaca_data_cleaned.json`）。
    * **产出**:
        1.  一个完整的微调脚本。
        2.  保存下来的LoRA适配器权重。
        3.  一个推理脚本，能加载原模型和你的LoRA权重，让你能与自己微调后的模型进行对话。
    * **检验标准**: 你能成功让模型学会一些新知识或新的对话风格，并能解释微调代码中每一步的作用。

---

### **第三阶段：拓宽视野与准备面试 (持续4周)**

**目标**: 了解前沿架构，为面试中展现技术广度做准备，并整理项目、模拟面试。

* **理论学习 (Week 11-12):**
    * **前沿架构 (Mamba/Hyena)**:
        * **目标**: 理解概念，而非实现。
        * **任务**: 阅读高质量的技术博客和解读文章。你需要回答三个问题：它们解决了Transformer的什么痛点？（二次方复杂度）；它们的核心思想是什么？（状态空间模型/长卷积）；它们有什么优缺点？
    * **分布式训练入门**:
        * **目标**: 理解基本概念。
        * **任务**: 学习****数据并行(Data Parallelism)和模型并行(Model Parallelism)****的核心思想。知道`DeepSpeed ZeRO`是为了解决什么问题（显存冗余）。
    * **其他微调技术**: 快速浏览`Prompt Tuning`, `P-Tuning`的介绍，知道它们和LoRA的区别。

* **项目实践与面试准备 (Week 13-14):**
    * **建立作品集**: 将你在阶段一和阶段二中完成的项目代码整理好，写上清晰的`README.md`说明文档，上传到你的GitHub。这是你最重要的简历内容。
    * **准备简历**: 根据职位描述，突出你在这些项目中的贡献和学到的技能。
    * **模拟面试**: 找同学或朋友进行模拟面试。重点练习如何清晰地介绍你的项目，特别是你遇到的困难以及如何解决的。练习回答“为什么选择LoRA？”“Transformer的瓶颈是什么？”等问题。
    * **(选修)云平台实践**: 如果时间和预算允许，可以花几十块钱在`AutoDL`上租一个双卡GPU服务器，把你写的LoRA微调脚本用`PyTorch DDP`（数据并行）模式跑一遍。这将成为你简历上一个巨大的亮点。
